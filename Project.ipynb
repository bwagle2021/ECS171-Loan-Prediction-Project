{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Logistic Regression Model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "source": [
    "\r\n",
    "import pandas as pd\r\n",
    "import numpy as np\r\n",
    "from sklearn import preprocessing\r\n",
    "\r\n",
    "trainingDf = pd.read_csv('./train_dataset.csv.xls')\r\n",
    "\r\n",
    "print(trainingDf.head())"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "    Loan_ID Gender Married Dependents     Education Self_Employed  \\\n",
      "0  LP001002   Male      No          0      Graduate            No   \n",
      "1  LP001003   Male     Yes          1      Graduate            No   \n",
      "2  LP001005   Male     Yes          0      Graduate           Yes   \n",
      "3  LP001006   Male     Yes          0  Not Graduate            No   \n",
      "4  LP001008   Male      No          0      Graduate            No   \n",
      "\n",
      "   ApplicantIncome  CoapplicantIncome  LoanAmount  Loan_Amount_Term  \\\n",
      "0             5849                0.0         NaN             360.0   \n",
      "1             4583             1508.0       128.0             360.0   \n",
      "2             3000                0.0        66.0             360.0   \n",
      "3             2583             2358.0       120.0             360.0   \n",
      "4             6000                0.0       141.0             360.0   \n",
      "\n",
      "   Credit_History Property_Area Loan_Status  \n",
      "0             1.0         Urban           Y  \n",
      "1             1.0         Rural           N  \n",
      "2             1.0         Urban           Y  \n",
      "3             1.0         Urban           Y  \n",
      "4             1.0         Urban           Y  \n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "source": [
    "print(\"============COLUMNS WITH EMPTY VALUE=============\")\r\n",
    "print(trainingDf.columns[trainingDf.isna().any()].tolist())\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "============COLUMNS WITH EMPTY VALUE=============\n",
      "['Gender', 'Married', 'Dependents', 'Self_Employed', 'LoanAmount', 'Loan_Amount_Term', 'Credit_History']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "source": [
    "#Dropping loan_id\r\n",
    "trainingDf = trainingDf.drop(columns=['Loan_ID'])\r\n",
    "\r\n",
    "#Populating rows with null data with default values\r\n",
    "trainingDf['Gender'] = trainingDf['Gender'].fillna('unknown')\r\n",
    "trainingDf['Married'] = trainingDf['Married'].fillna('unknown')\r\n",
    "trainingDf['Self_Employed'] = trainingDf['Self_Employed'].fillna('unknown')\r\n",
    "trainingDf['Dependents'] = trainingDf['Dependents'].fillna(0)\r\n",
    "\r\n",
    "#Dropping rows with empty values for following columns 'LoanAmount', 'Loan_Amount_Term', 'Credit_History'\r\n",
    "trainingDf = trainingDf[trainingDf['LoanAmount'].notna()]\r\n",
    "trainingDf = trainingDf[trainingDf['Loan_Amount_Term'].notna()]\r\n",
    "trainingDf = trainingDf[trainingDf['Credit_History'].notna()]\r\n",
    "\r\n",
    "trainingDf['Dependents'].replace(\r\n",
    "    to_replace=['3+'],\r\n",
    "    value='4',\r\n",
    "    inplace=True\r\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "source": [
    "\r\n",
    "print(\"============COLUMNS WITH EMPTY VALUE=============\")\r\n",
    "print(trainingDf.columns[trainingDf.isna().any()].tolist())\r\n",
    "print(\"Remaining rows after dropping data:\", len(trainingDf.index))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "============COLUMNS WITH EMPTY VALUE=============\n",
      "[]\n",
      "Remaining rows after dropping data: 529\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "source": [
    "print(\"============UNIQUE VALUE FOR CATEGORICAL DATA=============\")\r\n",
    "print(\"Gender: \", trainingDf['Gender'].unique())\r\n",
    "print(\"Married: \", trainingDf['Married'].unique())\r\n",
    "print(\"Education: \", trainingDf['Education'].unique())\r\n",
    "print(\"Self_Employed: \", trainingDf['Self_Employed'].unique())\r\n",
    "print(\"Property_Area: \", trainingDf['Property_Area'].unique())\r\n",
    "print('\\n')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "============UNIQUE VALUE FOR CATEGORICAL DATA=============\n",
      "Gender:  ['Male' 'Female' 'unknown']\n",
      "Married:  ['Yes' 'No' 'unknown']\n",
      "Education:  ['Graduate' 'Not Graduate']\n",
      "Self_Employed:  ['No' 'Yes' 'unknown']\n",
      "Property_Area:  ['Rural' 'Urban' 'Semiurban']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "source": [
    "from sklearn import preprocessing\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "\r\n",
    "\r\n",
    "train, test = train_test_split(trainingDf, test_size=0.2, random_state=23)\r\n",
    "print(train.head())\r\n",
    "X_train, y_train = train.copy().drop(columns=['Loan_Status']), train['Loan_Status']\r\n",
    "X_test, y_test = test.copy().drop(columns=['Loan_Status']), test['Loan_Status']\r\n",
    "\r\n",
    "Categorical_columns = [\"Gender\", \"Married\", \"Education\", \"Self_Employed\", \"Property_Area\"]\r\n",
    "Numerical_columns = [\"Dependents\", \"ApplicantIncome\", \"CoapplicantIncome\", \"LoanAmount\", \"Loan_Amount_Term\", \"Credit_History\"]\r\n",
    "\r\n",
    "Categorical_X_train = X_train[Categorical_columns]\r\n",
    "Categorical_X_test = X_test[Categorical_columns]\r\n",
    "\r\n",
    "Numerical_X_train = X_train[Numerical_columns]\r\n",
    "Numerical_X_test = X_test[Numerical_columns]\r\n",
    "\r\n",
    "encoder = preprocessing.OneHotEncoder()\r\n",
    "encoder.fit(Categorical_X_train)\r\n",
    "Categorical_X_train_encoded = encoder.transform(Categorical_X_train).toarray()\r\n",
    "Categorical_X_test_encoded = encoder.transform(Categorical_X_test).toarray()\r\n",
    "\r\n",
    "print(Numerical_X_train.head())\r\n",
    "\r\n",
    "standard_Scaler = preprocessing.MinMaxScaler()\r\n",
    "standard_Scaler.fit(Numerical_X_train)\r\n",
    "Numerical_X_train_encoded = standard_Scaler.transform(Numerical_X_train)\r\n",
    "Numerical_X_test_encoded = standard_Scaler.transform(Numerical_X_test)\r\n",
    "\r\n",
    "y_encoder = preprocessing.LabelEncoder()\r\n",
    "y_encoder.fit(y_train)\r\n",
    "Y_train_encoded = y_encoder.transform(y_train)\r\n",
    "Y_test_encoded = y_encoder.transform(y_test)\r\n",
    "\r\n",
    "Combined_X_train_encoded = np.concatenate((Categorical_X_train_encoded, Numerical_X_train_encoded), axis=1)\r\n",
    "#print(Combined_X_train_encoded)\r\n",
    "Combined_X_test_encoded = np.concatenate((Categorical_X_test_encoded, Numerical_X_test_encoded), axis=1)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "     Gender Married Dependents     Education Self_Employed  ApplicantIncome  \\\n",
      "330    Male      No          1      Graduate            No             4384   \n",
      "410  Female      No          1  Not Graduate           Yes             3867   \n",
      "217    Male     Yes          0      Graduate            No             3727   \n",
      "143    Male     Yes          0      Graduate            No             2698   \n",
      "525    Male     Yes          2      Graduate           Yes            17500   \n",
      "\n",
      "     CoapplicantIncome  LoanAmount  Loan_Amount_Term  Credit_History  \\\n",
      "330             1793.0       117.0             360.0             1.0   \n",
      "410                0.0        62.0             360.0             1.0   \n",
      "217             1775.0       131.0             360.0             1.0   \n",
      "143             2034.0       122.0             360.0             1.0   \n",
      "525                0.0       400.0             360.0             1.0   \n",
      "\n",
      "    Property_Area Loan_Status  \n",
      "330         Urban           Y  \n",
      "410     Semiurban           N  \n",
      "217     Semiurban           Y  \n",
      "143     Semiurban           Y  \n",
      "525         Rural           Y  \n",
      "    Dependents  ApplicantIncome  CoapplicantIncome  LoanAmount  \\\n",
      "330          1             4384             1793.0       117.0   \n",
      "410          1             3867                0.0        62.0   \n",
      "217          0             3727             1775.0       131.0   \n",
      "143          0             2698             2034.0       122.0   \n",
      "525          2            17500                0.0       400.0   \n",
      "\n",
      "     Loan_Amount_Term  Credit_History  \n",
      "330             360.0             1.0  \n",
      "410             360.0             1.0  \n",
      "217             360.0             1.0  \n",
      "143             360.0             1.0  \n",
      "525             360.0             1.0  \n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1) Logistic Regression Model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "source": [
    "#model using Logistic Regression\r\n",
    "from sklearn.linear_model import LogisticRegression\r\n",
    "from sklearn.metrics import classification_report, confusion_matrix, multilabel_confusion_matrix\r\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score, precision_score, recall_score\r\n",
    "import numpy as np\r\n",
    "\r\n",
    "print(\"============LOGISTIC REGRESSION MODEL=============\")\r\n",
    "lr_model = LogisticRegression()\r\n",
    "lr_model.fit(Combined_X_train_encoded,Y_train_encoded)\r\n",
    "y_predicted = lr_model.predict(Combined_X_test_encoded)\r\n",
    "print(\"Accuracy is \", accuracy_score(y_predicted,Y_test_encoded))\r\n",
    "print(\"Mean Square Error : \", mean_squared_error(Y_test_encoded, y_predicted))\r\n",
    "print(confusion_matrix(y_predicted,Y_test_encoded))\r\n",
    "print(\"Classification Report : \")\r\n",
    "print(classification_report(Y_test_encoded, y_predicted))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "============LOGISTIC REGRESSION MODEL=============\n",
      "Accuracy is  0.8207547169811321\n",
      "Mean Square Error :  0.1792452830188679\n",
      "[[23  0]\n",
      " [19 64]]\n",
      "Classification Report : \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.55      0.71        42\n",
      "           1       0.77      1.00      0.87        64\n",
      "\n",
      "    accuracy                           0.82       106\n",
      "   macro avg       0.89      0.77      0.79       106\n",
      "weighted avg       0.86      0.82      0.81       106\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2) 10-fold cross validation to generalize the Logistic Regression Model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "source": [
    "# To find list of accuracy and MSE values of the model using K-fold cross validation\r\n",
    "\r\n",
    "from sklearn.model_selection import KFold\r\n",
    "\r\n",
    "print(\"============LOGISTIC REGRESSION MODEL USING 10-FOLD CROSS VALIDATION=============\")\r\n",
    "kf = KFold(n_splits=10)\r\n",
    "X = trainingDf.copy().drop(columns=['Loan_Status'])\r\n",
    "y = trainingDf['Loan_Status']\r\n",
    "\r\n",
    "#re-split the processed data into X and y then encode before doing 10-cross validation\r\n",
    "Categorical_X = X[Categorical_columns]\r\n",
    "#print(Categorical_X)\r\n",
    "Numerical_X = X[Numerical_columns]\r\n",
    "\r\n",
    "encoder = preprocessing.OneHotEncoder()\r\n",
    "encoder.fit(Categorical_X)\r\n",
    "Categorical_X_encoded = encoder.transform(Categorical_X).toarray()\r\n",
    "\r\n",
    "#print(Categorical_X_encoded)\r\n",
    "\r\n",
    "standard_Scaler.fit(Numerical_X)\r\n",
    "Numerical_X_encoded = standard_Scaler.transform(Numerical_X)\r\n",
    "#print(Numerical_X_encoded)\r\n",
    "\r\n",
    "y_encoder.fit(y)\r\n",
    "Y_encoded = y_encoder.transform(y)\r\n",
    "\r\n",
    "Combined_X_encoded = np.concatenate((Categorical_X_encoded, Numerical_X_encoded), axis=1)\r\n",
    "\r\n",
    "#print(Combined_X_encoded)\r\n",
    "\r\n",
    "idx = 0\r\n",
    "for train_indices, test_indices in kf.split(Combined_X_encoded):\r\n",
    "    start_train, stop_train = train_indices[0], train_indices[-1]+1\r\n",
    "    start_test, stop_test = test_indices[0], test_indices[-1]+1\r\n",
    "    \r\n",
    "    lr_model.fit(Combined_X_encoded[start_train:stop_train], Y_encoded[start_train:stop_train])\r\n",
    "    pred = lr_model.predict(Combined_X_encoded[start_test:stop_test])\r\n",
    "\r\n",
    "    idx+=1\r\n",
    "    print(\"Accuracy for batch \", idx, \" : \", accuracy_score(Y_encoded[start_test:stop_test], pred))\r\n",
    "    print(\"Mean Square Error for batch \", idx, \" : \", mean_squared_error(Y_encoded[start_test:stop_test], pred))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "============LOGISTIC REGRESSION MODEL USING 10-FOLD CROSS VALIDATION=============\n",
      "Accuracy for batch  1  :  0.7735849056603774\n",
      "Mean Square Error for batch  1  :  0.22641509433962265\n",
      "Accuracy for batch  2  :  0.8490566037735849\n",
      "Mean Square Error for batch  2  :  0.1509433962264151\n",
      "Accuracy for batch  3  :  0.7547169811320755\n",
      "Mean Square Error for batch  3  :  0.24528301886792453\n",
      "Accuracy for batch  4  :  0.7735849056603774\n",
      "Mean Square Error for batch  4  :  0.22641509433962265\n",
      "Accuracy for batch  5  :  0.8301886792452831\n",
      "Mean Square Error for batch  5  :  0.16981132075471697\n",
      "Accuracy for batch  6  :  0.7547169811320755\n",
      "Mean Square Error for batch  6  :  0.24528301886792453\n",
      "Accuracy for batch  7  :  0.8490566037735849\n",
      "Mean Square Error for batch  7  :  0.1509433962264151\n",
      "Accuracy for batch  8  :  0.8679245283018868\n",
      "Mean Square Error for batch  8  :  0.1320754716981132\n",
      "Accuracy for batch  9  :  0.8301886792452831\n",
      "Mean Square Error for batch  9  :  0.16981132075471697\n",
      "Accuracy for batch  10  :  0.8653846153846154\n",
      "Mean Square Error for batch  10  :  0.1346153846153846\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3) Na誰ve Bayes Classifier for Numerical Attributes"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "source": [
    "#Calssify using Naive Bayes for numerical attributes \r\n",
    "from sklearn.naive_bayes import GaussianNB\r\n",
    "from sklearn.metrics import classification_report \r\n",
    "\r\n",
    "print(\"============NAIVE BAYES CLASSIFIER FOR NUMERICAL ATTRIBUTES=============\")\r\n",
    "\r\n",
    "GNB = GaussianNB()\r\n",
    "\r\n",
    "GNB.fit(Numerical_X_train_encoded, Y_train_encoded)\r\n",
    "\r\n",
    "print(\"Classification report for train datasets:\")\r\n",
    "print(classification_report(Y_train_encoded, GNB.predict(Numerical_X_train_encoded)))\r\n",
    "print(\"***************************************************************************\")\r\n",
    "print(\"Classification report for test datasets:\")\r\n",
    "print(classification_report(Y_test_encoded, GNB.predict(Numerical_X_test_encoded)))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "============NAIVE BAYES CLASSIFIER FOR NUMERICAL ATTRIBUTES=============\n",
      "Classification report for train datasets:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.47      0.58       121\n",
      "           1       0.82      0.94      0.88       302\n",
      "\n",
      "    accuracy                           0.81       423\n",
      "   macro avg       0.79      0.71      0.73       423\n",
      "weighted avg       0.80      0.81      0.79       423\n",
      "\n",
      "***************************************************************************\n",
      "Classification report for test datasets:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.55      0.69        42\n",
      "           1       0.77      0.97      0.86        64\n",
      "\n",
      "    accuracy                           0.80       106\n",
      "   macro avg       0.84      0.76      0.77       106\n",
      "weighted avg       0.83      0.80      0.79       106\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4) 10-fold cross validation to generalize the Na誰ve Bayes Classifier for Numerical Attributes"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "source": [
    "# To find list of accuracy and MSE values of the model using K-fold cross validation\r\n",
    "\r\n",
    "from sklearn.model_selection import KFold\r\n",
    "\r\n",
    "kf = KFold(n_splits=10)\r\n",
    "\r\n",
    "print(\"============NAIVE BAYES CLASSIFIER FOR NUMERICAL ATTRIBUTES USING 10-FOLD CROSS VALIDATION=============\")\r\n",
    "\r\n",
    "idx = 0\r\n",
    "for train_indices, test_indices in kf.split(Numerical_X_encoded):\r\n",
    "    start_train, stop_train = train_indices[0], train_indices[-1]+1\r\n",
    "    start_test, stop_test = test_indices[0], test_indices[-1]+1\r\n",
    "    \r\n",
    "    GNB.fit(Numerical_X_encoded[start_train:stop_train], Y_encoded[start_train:stop_train])\r\n",
    "    pred = GNB.predict(Numerical_X_encoded[start_test:stop_test])\r\n",
    "\r\n",
    "    idx+=1\r\n",
    "    print(\"Accuracy for batch \", idx, \" : \", accuracy_score(Y_encoded[start_test:stop_test], pred))\r\n",
    "    print(\"Mean Square Error for batch \", idx, \" : \", mean_squared_error(Y_encoded[start_test:stop_test], pred))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "============NAIVE BAYES CLASSIFIER FOR NUMERICAL ATTRIBUTES USING 10-FOLD CROSS VALIDATION=============\n",
      "Accuracy for batch  1  :  0.7547169811320755\n",
      "Mean Square Error for batch  1  :  0.24528301886792453\n",
      "Accuracy for batch  2  :  0.8490566037735849\n",
      "Mean Square Error for batch  2  :  0.1509433962264151\n",
      "Accuracy for batch  3  :  0.7547169811320755\n",
      "Mean Square Error for batch  3  :  0.24528301886792453\n",
      "Accuracy for batch  4  :  0.7547169811320755\n",
      "Mean Square Error for batch  4  :  0.24528301886792453\n",
      "Accuracy for batch  5  :  0.8113207547169812\n",
      "Mean Square Error for batch  5  :  0.18867924528301888\n",
      "Accuracy for batch  6  :  0.7547169811320755\n",
      "Mean Square Error for batch  6  :  0.24528301886792453\n",
      "Accuracy for batch  7  :  0.8301886792452831\n",
      "Mean Square Error for batch  7  :  0.16981132075471697\n",
      "Accuracy for batch  8  :  0.8490566037735849\n",
      "Mean Square Error for batch  8  :  0.1509433962264151\n",
      "Accuracy for batch  9  :  0.8113207547169812\n",
      "Mean Square Error for batch  9  :  0.18867924528301888\n",
      "Accuracy for batch  10  :  0.8461538461538461\n",
      "Mean Square Error for batch  10  :  0.15384615384615385\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 5) Na誰ve Bayes Classifier for Categorical Attributes"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "source": [
    "from sklearn.metrics import classification_report\r\n",
    "from sklearn.naive_bayes import CategoricalNB\r\n",
    "\r\n",
    "print(\"============NAIVE BAYES CLASSIFIER FOR CATEGORICAL ATTRIBUTES=============\")\r\n",
    "\r\n",
    "NB = CategoricalNB() \r\n",
    "\r\n",
    "NB.fit(Categorical_X_train_encoded, Y_train_encoded)\r\n",
    "print(classification_report(Y_test_encoded, NB.predict(Categorical_X_test_encoded)))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "============NAIVE BAYES CLASSIFIER FOR CATEGORICAL ATTRIBUTES=============\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.47      0.19      0.27        42\n",
      "           1       0.62      0.86      0.72        64\n",
      "\n",
      "    accuracy                           0.59       106\n",
      "   macro avg       0.54      0.52      0.50       106\n",
      "weighted avg       0.56      0.59      0.54       106\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 6) 10-fold cross validation to generalize the Na誰ve Bayes Classifier for Categorical Attributes"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "source": [
    "from sklearn.model_selection import KFold\r\n",
    "\r\n",
    "print(\"============NAIVE BAYES CLASSIFIER FOR CATEGORICAL ATTRIBUTES USING 10-FOLD CROSS VALIDATION=============\")\r\n",
    "\r\n",
    "kf = KFold(n_splits=10)\r\n",
    "\r\n",
    "idx = 0\r\n",
    "for train_indices, test_indices in kf.split(Categorical_X_encoded):\r\n",
    "    start_train, stop_train = train_indices[0], train_indices[-1]+1\r\n",
    "    start_test, stop_test = test_indices[0], test_indices[-1]+1\r\n",
    "    \r\n",
    "    GNB.fit(Categorical_X_encoded[start_train:stop_train], Y_encoded[start_train:stop_train])\r\n",
    "    pred = GNB.predict(Categorical_X_encoded[start_test:stop_test])\r\n",
    "\r\n",
    "    idx+=1\r\n",
    "    print(\"Accuracy for batch \", idx, \" : \", accuracy_score(Y_encoded[start_test:stop_test], pred))\r\n",
    "    print(\"Mean Square Error for batch \", idx, \" : \", mean_squared_error(Y_encoded[start_test:stop_test], pred))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "============NAIVE BAYES CLASSIFIER FOR CATEGORICAL ATTRIBUTES USING 10-FOLD CROSS VALIDATION=============\n",
      "Accuracy for batch  1  :  0.41509433962264153\n",
      "Mean Square Error for batch  1  :  0.5849056603773585\n",
      "Accuracy for batch  2  :  0.2830188679245283\n",
      "Mean Square Error for batch  2  :  0.7169811320754716\n",
      "Accuracy for batch  3  :  0.41509433962264153\n",
      "Mean Square Error for batch  3  :  0.5849056603773585\n",
      "Accuracy for batch  4  :  0.2830188679245283\n",
      "Mean Square Error for batch  4  :  0.7169811320754716\n",
      "Accuracy for batch  5  :  0.3018867924528302\n",
      "Mean Square Error for batch  5  :  0.6981132075471698\n",
      "Accuracy for batch  6  :  0.3584905660377358\n",
      "Mean Square Error for batch  6  :  0.6415094339622641\n",
      "Accuracy for batch  7  :  0.37735849056603776\n",
      "Mean Square Error for batch  7  :  0.6226415094339622\n",
      "Accuracy for batch  8  :  0.33962264150943394\n",
      "Mean Square Error for batch  8  :  0.660377358490566\n",
      "Accuracy for batch  9  :  0.32075471698113206\n",
      "Mean Square Error for batch  9  :  0.6792452830188679\n",
      "Accuracy for batch  10  :  0.34615384615384615\n",
      "Mean Square Error for batch  10  :  0.6538461538461539\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 7) Support Vector Machines\r\n",
    "Support Vector Machines are supervised learning algorithms that are well supported for classification and regression problems due to their significant accuracy. The goal of an SVM is to find decision boundaries, also known as hyperplanes, within an N-dimension plane (where N is the number of features) that help classify the samples given. The position and orientation of the hyperplane is influenced by support vectors, data points that are closer to the hyperplane, which help build the model. Since the loan prediction is a binary classification problem, using an SVM is a good approach to creating a predictive model.\r\n",
    "\r\n",
    "The first SVM uses a Gaussian (or radial basis function) kernel which is a non-linear SVM that uses the euclidian distance between two points to help classify the samples. The second SVM is a linear SVM which removes all of the data points that are not closer to the hyperplane, resulting with only support vectors.\r\n",
    "\r\n",
    "Both of the SVMs have similar results, but the Linear SVM is slightly better. This could be due to the large dataset which increases the complexity of non-linear SVMs. On the other hand, the linear SVM will not increase in complexity with larger datasets."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "source": [
    "from sklearn.svm import SVC, LinearSVC\r\n",
    "\r\n",
    "print(\"============ RBF Kernal =============\")\r\n",
    "clf_rbf = SVC(kernel='rbf')\r\n",
    "clf_rbf.fit(Combined_X_train_encoded, Y_train_encoded)\r\n",
    "print(\"Training: \")\r\n",
    "print(classification_report(Y_train_encoded, clf_rbf.predict(Combined_X_train_encoded)))\r\n",
    "print(\"Testing: \")\r\n",
    "print(classification_report(Y_test_encoded, clf_rbf.predict(Combined_X_test_encoded)))\r\n",
    "\r\n",
    "print(\"============ Linear Kernal =============\")\r\n",
    "clf_lin = LinearSVC(dual=False)\r\n",
    "clf_lin.fit(Combined_X_train_encoded, Y_train_encoded)\r\n",
    "print(\"Training: \")\r\n",
    "print(classification_report(Y_train_encoded, clf_lin.predict(Combined_X_train_encoded)))\r\n",
    "print(\"Testing: \")\r\n",
    "print(classification_report(Y_test_encoded, clf_lin.predict(Combined_X_test_encoded)))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "============ RBF Kernal =============\n",
      "0.8132387706855791\n",
      "Training: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.40      0.55       121\n",
      "           1       0.80      0.98      0.88       302\n",
      "\n",
      "    accuracy                           0.81       423\n",
      "   macro avg       0.84      0.69      0.72       423\n",
      "weighted avg       0.82      0.81      0.79       423\n",
      "\n",
      "Testing: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.55      0.71        42\n",
      "           1       0.77      1.00      0.87        64\n",
      "\n",
      "    accuracy                           0.82       106\n",
      "   macro avg       0.89      0.77      0.79       106\n",
      "weighted avg       0.86      0.82      0.81       106\n",
      "\n",
      "============ Linear Kernal =============\n",
      "0.8156028368794326\n",
      "Training: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.41      0.56       121\n",
      "           1       0.81      0.98      0.88       302\n",
      "\n",
      "    accuracy                           0.82       423\n",
      "   macro avg       0.84      0.70      0.72       423\n",
      "weighted avg       0.83      0.82      0.79       423\n",
      "\n",
      "Testing: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.55      0.71        42\n",
      "           1       0.77      1.00      0.87        64\n",
      "\n",
      "    accuracy                           0.82       106\n",
      "   macro avg       0.89      0.77      0.79       106\n",
      "weighted avg       0.86      0.82      0.81       106\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 8) Artificial Neural Network"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\r\n",
    "from sklearn.neural_network import MLPClassifier\r\n",
    "\r\n",
    "clf = MLPClassifier(hidden_layer_sizes=(12,4), activation='logistic', solver = 'sgd', max_iter = 500, learning_rate_init = 0.3, random_state=1)\r\n",
    "clf.fit(Combined_X_train_encoded, Y_train_encoded)\r\n",
    "cm = confusion_matrix(Y_test_encoded, clf.predict(Combined_X_test_encoded))\r\n",
    "ConfusionMatrixDisplay(cm).plot()\r\n",
    "\r\n",
    "print(\"Training: \")\r\n",
    "print(classification_report(Y_train_encoded, clf.predict(Combined_X_train_encoded), zero_division=0))\r\n",
    "print(\"Testing: \")\r\n",
    "print(classification_report(Y_test_encoded, clf.predict(Combined_X_test_encoded), zero_division=0))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       121\n",
      "           1       0.71      1.00      0.83       302\n",
      "\n",
      "    accuracy                           0.71       423\n",
      "   macro avg       0.36      0.50      0.42       423\n",
      "weighted avg       0.51      0.71      0.59       423\n",
      "\n",
      "Testing: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        42\n",
      "           1       0.60      1.00      0.75        64\n",
      "\n",
      "    accuracy                           0.60       106\n",
      "   macro avg       0.30      0.50      0.38       106\n",
      "weighted avg       0.36      0.60      0.45       106\n",
      "\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "interpreter": {
   "hash": "7ff52d1df0f22be24dc1e8e4bb667511948f02652dc42c287babb40e2a0977d0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}