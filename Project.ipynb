{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 181,
   "source": [
    "\r\n",
    "import pandas as pd\r\n",
    "import numpy as np\r\n",
    "from sklearn import preprocessing\r\n",
    "\r\n",
    "trainingDf = pd.read_csv('./train_dataset.csv.xls')\r\n",
    "\r\n",
    "print(trainingDf.head())"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "    Loan_ID Gender Married Dependents     Education Self_Employed  \\\n",
      "0  LP001002   Male      No          0      Graduate            No   \n",
      "1  LP001003   Male     Yes          1      Graduate            No   \n",
      "2  LP001005   Male     Yes          0      Graduate           Yes   \n",
      "3  LP001006   Male     Yes          0  Not Graduate            No   \n",
      "4  LP001008   Male      No          0      Graduate            No   \n",
      "\n",
      "   ApplicantIncome  CoapplicantIncome  LoanAmount  Loan_Amount_Term  \\\n",
      "0             5849                0.0         NaN             360.0   \n",
      "1             4583             1508.0       128.0             360.0   \n",
      "2             3000                0.0        66.0             360.0   \n",
      "3             2583             2358.0       120.0             360.0   \n",
      "4             6000                0.0       141.0             360.0   \n",
      "\n",
      "   Credit_History Property_Area Loan_Status  \n",
      "0             1.0         Urban           Y  \n",
      "1             1.0         Rural           N  \n",
      "2             1.0         Urban           Y  \n",
      "3             1.0         Urban           Y  \n",
      "4             1.0         Urban           Y  \n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "source": [
    "print(\"============COLUMNS WITH EMPTY VALUE=============\")\r\n",
    "print(trainingDf.columns[trainingDf.isna().any()].tolist())\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "============COLUMNS WITH EMPTY VALUE=============\n",
      "['Gender', 'Married', 'Dependents', 'Self_Employed', 'LoanAmount', 'Loan_Amount_Term', 'Credit_History']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "source": [
    "#Dropping loan_id\r\n",
    "trainingDf = trainingDf.drop(columns=['Loan_ID'])\r\n",
    "\r\n",
    "#Populating rows with null data with default values\r\n",
    "trainingDf['Gender'] = trainingDf['Gender'].fillna('unknown')\r\n",
    "trainingDf['Married'] = trainingDf['Married'].fillna('unknown')\r\n",
    "trainingDf['Self_Employed'] = trainingDf['Self_Employed'].fillna('unknown')\r\n",
    "trainingDf['Dependents'] = trainingDf['Dependents'].fillna(0)\r\n",
    "\r\n",
    "#Dropping rows with empty values for following columns 'LoanAmount', 'Loan_Amount_Term', 'Credit_History'\r\n",
    "trainingDf = trainingDf[trainingDf['LoanAmount'].notna()]\r\n",
    "trainingDf = trainingDf[trainingDf['Loan_Amount_Term'].notna()]\r\n",
    "trainingDf = trainingDf[trainingDf['Credit_History'].notna()]\r\n",
    "\r\n",
    "trainingDf['Dependents'].replace(\r\n",
    "    to_replace=['3+'],\r\n",
    "    value='4',\r\n",
    "    inplace=True\r\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "source": [
    "\r\n",
    "print(\"============COLUMNS WITH EMPTY VALUE=============\")\r\n",
    "print(trainingDf.columns[trainingDf.isna().any()].tolist())\r\n",
    "print(\"Remaining rows after dropping data:\", len(trainingDf.index))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "============COLUMNS WITH EMPTY VALUE=============\n",
      "[]\n",
      "Remaining rows after dropping data: 529\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "source": [
    "print(\"============UNIQUE VALUE FOR CATEGORICAL DATA=============\")\r\n",
    "print(\"Gender: \", trainingDf['Gender'].unique())\r\n",
    "print(\"Married: \", trainingDf['Married'].unique())\r\n",
    "print(\"Education: \", trainingDf['Education'].unique())\r\n",
    "print(\"Self_Employed: \", trainingDf['Self_Employed'].unique())\r\n",
    "print(\"Property_Area: \", trainingDf['Property_Area'].unique())\r\n",
    "print('\\n')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "============UNIQUE VALUE FOR CATEGORICAL DATA=============\n",
      "Gender:  ['Male' 'Female' 'unknown']\n",
      "Married:  ['Yes' 'No' 'unknown']\n",
      "Education:  ['Graduate' 'Not Graduate']\n",
      "Self_Employed:  ['No' 'Yes' 'unknown']\n",
      "Property_Area:  ['Rural' 'Urban' 'Semiurban']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "source": [
    "from sklearn import preprocessing\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "\r\n",
    "\r\n",
    "train, test = train_test_split(trainingDf, test_size=0.2, random_state=23)\r\n",
    "print(train.head())\r\n",
    "X_train, y_train = train.copy().drop(columns=['Loan_Status']), train['Loan_Status']\r\n",
    "X_test, y_test = test.copy().drop(columns=['Loan_Status']), test['Loan_Status']\r\n",
    "\r\n",
    "Categorical_columns = [\"Gender\", \"Married\", \"Education\", \"Self_Employed\", \"Property_Area\"]\r\n",
    "Numerical_columns = [\"Dependents\", \"ApplicantIncome\", \"CoapplicantIncome\", \"LoanAmount\", \"Loan_Amount_Term\", \"Credit_History\"]\r\n",
    "\r\n",
    "Categorical_X_train = X_train[Categorical_columns]\r\n",
    "Categorical_X_test = X_test[Categorical_columns]\r\n",
    "\r\n",
    "Numerical_X_train = X_train[Numerical_columns]\r\n",
    "Numerical_X_test = X_test[Numerical_columns]\r\n",
    "\r\n",
    "encoder = preprocessing.OneHotEncoder()\r\n",
    "encoder.fit(Categorical_X_train)\r\n",
    "Categorical_X_train_encoded = encoder.transform(Categorical_X_train).toarray()\r\n",
    "Categorical_X_test_encoded = encoder.transform(Categorical_X_test).toarray()\r\n",
    "\r\n",
    "print(Numerical_X_train.head())\r\n",
    "\r\n",
    "standard_Scaler = preprocessing.MinMaxScaler()\r\n",
    "standard_Scaler.fit(Numerical_X_train)\r\n",
    "Numerical_X_train_encoded = standard_Scaler.transform(Numerical_X_train)\r\n",
    "Numerical_X_test_encoded = standard_Scaler.transform(Numerical_X_test)\r\n",
    "\r\n",
    "y_encoder = preprocessing.LabelEncoder()\r\n",
    "y_encoder.fit(y_train)\r\n",
    "Y_train_encoded = y_encoder.transform(y_train)\r\n",
    "Y_test_encoded = y_encoder.transform(y_test)\r\n",
    "\r\n",
    "Combined_X_train_encoded = np.concatenate((Categorical_X_train_encoded, Numerical_X_train_encoded), axis=1)\r\n",
    "#print(Combined_X_train_encoded)\r\n",
    "Combined_X_test_encoded = np.concatenate((Categorical_X_test_encoded, Numerical_X_test_encoded), axis=1)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "     Gender Married Dependents     Education Self_Employed  ApplicantIncome  \\\n",
      "330    Male      No          1      Graduate            No             4384   \n",
      "410  Female      No          1  Not Graduate           Yes             3867   \n",
      "217    Male     Yes          0      Graduate            No             3727   \n",
      "143    Male     Yes          0      Graduate            No             2698   \n",
      "525    Male     Yes          2      Graduate           Yes            17500   \n",
      "\n",
      "     CoapplicantIncome  LoanAmount  Loan_Amount_Term  Credit_History  \\\n",
      "330             1793.0       117.0             360.0             1.0   \n",
      "410                0.0        62.0             360.0             1.0   \n",
      "217             1775.0       131.0             360.0             1.0   \n",
      "143             2034.0       122.0             360.0             1.0   \n",
      "525                0.0       400.0             360.0             1.0   \n",
      "\n",
      "    Property_Area Loan_Status  \n",
      "330         Urban           Y  \n",
      "410     Semiurban           N  \n",
      "217     Semiurban           Y  \n",
      "143     Semiurban           Y  \n",
      "525         Rural           Y  \n",
      "    Dependents  ApplicantIncome  CoapplicantIncome  LoanAmount  \\\n",
      "330          1             4384             1793.0       117.0   \n",
      "410          1             3867                0.0        62.0   \n",
      "217          0             3727             1775.0       131.0   \n",
      "143          0             2698             2034.0       122.0   \n",
      "525          2            17500                0.0       400.0   \n",
      "\n",
      "     Loan_Amount_Term  Credit_History  \n",
      "330             360.0             1.0  \n",
      "410             360.0             1.0  \n",
      "217             360.0             1.0  \n",
      "143             360.0             1.0  \n",
      "525             360.0             1.0  \n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1) Logistic Regression Model\r\n",
    "\r\n",
    "Logistic Regression model is a predictive analysis and is a good fit for loan prediction because we want to predict a binary outcome: approved/not approved. Through encoding,\r\n",
    "the output is mapped as either 0 or 1. That means that we want to analyze the relationship between one dependent variable (the loan status) and multiple independent variable which are either categorical or numerical.This type of model solves the problem by using a logarithmic transformation on the dependent variable, allowing expression of linear regression in a logarithmic mode.\r\n",
    "\r\n",
    "Procedure: we created a Logistic Regression model `lr_model` which we then fitted over our training dataset (`lr_model.fit`). Then we used it to predict the output using our test dataset. We printed out metrics such as the accuracy of the model, the mean square error. We also printed the `confusion_matrix` which shows the true and false positives and the true and false negatives. We then used the `classification_report` method to print the precision, the recall, the f1-score and the support of the model. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "source": [
    "#model using Logistic Regression\r\n",
    "from sklearn.linear_model import LogisticRegression\r\n",
    "from sklearn.metrics import classification_report, confusion_matrix, multilabel_confusion_matrix\r\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score, precision_score, recall_score\r\n",
    "import numpy as np\r\n",
    "\r\n",
    "print(\"============LOGISTIC REGRESSION MODEL=============\")\r\n",
    "lr_model = LogisticRegression()\r\n",
    "lr_model.fit(Combined_X_train_encoded,Y_train_encoded)\r\n",
    "y_predicted = lr_model.predict(Combined_X_test_encoded)\r\n",
    "print(\"Accuracy is \", accuracy_score(y_predicted,Y_test_encoded))\r\n",
    "print(\"Mean Square Error : \", mean_squared_error(Y_test_encoded, y_predicted))\r\n",
    "print(confusion_matrix(y_predicted,Y_test_encoded))\r\n",
    "print(\"Classification Report : \")\r\n",
    "print(classification_report(Y_test_encoded, y_predicted))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "============LOGISTIC REGRESSION MODEL=============\n",
      "Accuracy is  0.8207547169811321\n",
      "Mean Square Error :  0.1792452830188679\n",
      "[[23  0]\n",
      " [19 64]]\n",
      "Classification Report : \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.55      0.71        42\n",
      "           1       0.77      1.00      0.87        64\n",
      "\n",
      "    accuracy                           0.82       106\n",
      "   macro avg       0.89      0.77      0.79       106\n",
      "weighted avg       0.86      0.82      0.81       106\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2) 10-fold cross validation to generalize the Logistic Regression Model\r\n",
    "\r\n",
    "We used 10-fold cross validation which is a resampling procedure that help avoiding overfitting or underfitting and generalizes the model by randomizing the dataset. The algorithm works by splitting the data iteratively into K partitions (batches - 10 in our case); then it builds K different models, each model being trained on the K-1 parts and tested on the Kth part. Then for each model, we print the accuracy of it and the MSE (mean square error). We can either have a set number of partitions, or choose K partitons by increasing the K value until there is no more change in the accuracy of the model or stop when the accuracy drops.\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "source": [
    "# To find list of accuracy and MSE values of the model using K-fold cross validation\r\n",
    "\r\n",
    "from sklearn.model_selection import KFold\r\n",
    "\r\n",
    "print(\"============LOGISTIC REGRESSION MODEL USING 10-FOLD CROSS VALIDATION=============\")\r\n",
    "kf = KFold(n_splits=10)\r\n",
    "X = trainingDf.copy().drop(columns=['Loan_Status'])\r\n",
    "y = trainingDf['Loan_Status']\r\n",
    "\r\n",
    "#re-split the processed data into X and y then encode before doing 10-cross validation\r\n",
    "Categorical_X = X[Categorical_columns]\r\n",
    "#print(Categorical_X)\r\n",
    "Numerical_X = X[Numerical_columns]\r\n",
    "\r\n",
    "encoder = preprocessing.OneHotEncoder()\r\n",
    "encoder.fit(Categorical_X)\r\n",
    "Categorical_X_encoded = encoder.transform(Categorical_X).toarray()\r\n",
    "\r\n",
    "#print(Categorical_X_encoded)\r\n",
    "\r\n",
    "standard_Scaler.fit(Numerical_X)\r\n",
    "Numerical_X_encoded = standard_Scaler.transform(Numerical_X)\r\n",
    "#print(Numerical_X_encoded)\r\n",
    "\r\n",
    "y_encoder.fit(y)\r\n",
    "Y_encoded = y_encoder.transform(y)\r\n",
    "\r\n",
    "Combined_X_encoded = np.concatenate((Categorical_X_encoded, Numerical_X_encoded), axis=1)\r\n",
    "\r\n",
    "#print(Combined_X_encoded)\r\n",
    "\r\n",
    "idx = 0\r\n",
    "for train_indices, test_indices in kf.split(Combined_X_encoded):\r\n",
    "    start_train, stop_train = train_indices[0], train_indices[-1]+1\r\n",
    "    start_test, stop_test = test_indices[0], test_indices[-1]+1\r\n",
    "    \r\n",
    "    lr_model.fit(Combined_X_encoded[start_train:stop_train], Y_encoded[start_train:stop_train])\r\n",
    "    pred = lr_model.predict(Combined_X_encoded[start_test:stop_test])\r\n",
    "\r\n",
    "    idx+=1\r\n",
    "    print(\"Accuracy for batch \", idx, \" : \", accuracy_score(Y_encoded[start_test:stop_test], pred))\r\n",
    "    print(\"Mean Square Error for batch \", idx, \" : \", mean_squared_error(Y_encoded[start_test:stop_test], pred))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "============LOGISTIC REGRESSION MODEL USING 10-FOLD CROSS VALIDATION=============\n",
      "Accuracy for batch  1  :  0.7735849056603774\n",
      "Mean Square Error for batch  1  :  0.22641509433962265\n",
      "Accuracy for batch  2  :  0.8490566037735849\n",
      "Mean Square Error for batch  2  :  0.1509433962264151\n",
      "Accuracy for batch  3  :  0.7547169811320755\n",
      "Mean Square Error for batch  3  :  0.24528301886792453\n",
      "Accuracy for batch  4  :  0.7735849056603774\n",
      "Mean Square Error for batch  4  :  0.22641509433962265\n",
      "Accuracy for batch  5  :  0.8301886792452831\n",
      "Mean Square Error for batch  5  :  0.16981132075471697\n",
      "Accuracy for batch  6  :  0.7547169811320755\n",
      "Mean Square Error for batch  6  :  0.24528301886792453\n",
      "Accuracy for batch  7  :  0.8490566037735849\n",
      "Mean Square Error for batch  7  :  0.1509433962264151\n",
      "Accuracy for batch  8  :  0.8679245283018868\n",
      "Mean Square Error for batch  8  :  0.1320754716981132\n",
      "Accuracy for batch  9  :  0.8301886792452831\n",
      "Mean Square Error for batch  9  :  0.16981132075471697\n",
      "Accuracy for batch  10  :  0.8653846153846154\n",
      "Mean Square Error for batch  10  :  0.1346153846153846\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3) Naïve Bayes Classifier for Numerical Attributes\r\n",
    "The Naïve Bayes Classifier is based on Bayes Theorem and it assumes a strong independence between the attributes; which means that the classifier assumes that the values of one feature is independent of the value of all other features. One advantage of using this type of classifier is that it needs only a small amount of training data such that it can estimate the parameters necessary for classification. \r\n",
    "Gaussian Naïve Bayes Classifier handles continuous data, in our case the numerical attributes. It has the assumption that the values associated with each class have a Gaussian(normal distribution). (https://iq.opengenus.org/gaussian-naive-bayes/)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "source": [
    "#Calssify using Naive Bayes for numerical attributes \r\n",
    "from sklearn.naive_bayes import GaussianNB\r\n",
    "from sklearn.metrics import classification_report \r\n",
    "\r\n",
    "print(\"============NAIVE BAYES CLASSIFIER FOR NUMERICAL ATTRIBUTES=============\")\r\n",
    "\r\n",
    "GNB = GaussianNB()\r\n",
    "\r\n",
    "GNB.fit(Numerical_X_train_encoded, Y_train_encoded)\r\n",
    "\r\n",
    "print(\"Classification report for train datasets:\")\r\n",
    "print(classification_report(Y_train_encoded, GNB.predict(Numerical_X_train_encoded)))\r\n",
    "print(\"***************************************************************************\")\r\n",
    "print(\"Classification report for test datasets:\")\r\n",
    "print(classification_report(Y_test_encoded, GNB.predict(Numerical_X_test_encoded)))\r\n",
    "print(\"Confusion matrix for Gaussian Naive Bayes Classifier: \")\r\n",
    "print(confusion_matrix(GNB.predict(Numerical_X_test_encoded),Y_test_encoded))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "============NAIVE BAYES CLASSIFIER FOR NUMERICAL ATTRIBUTES=============\n",
      "Classification report for train datasets:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.47      0.58       121\n",
      "           1       0.82      0.94      0.88       302\n",
      "\n",
      "    accuracy                           0.81       423\n",
      "   macro avg       0.79      0.71      0.73       423\n",
      "weighted avg       0.80      0.81      0.79       423\n",
      "\n",
      "***************************************************************************\n",
      "Classification report for test datasets:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.55      0.69        42\n",
      "           1       0.77      0.97      0.86        64\n",
      "\n",
      "    accuracy                           0.80       106\n",
      "   macro avg       0.84      0.76      0.77       106\n",
      "weighted avg       0.83      0.80      0.79       106\n",
      "\n",
      "Confusion matrix for Gaussian Naive Bayes Classifier: \n",
      "[[23  2]\n",
      " [19 62]]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4) 10-fold cross validation to generalize the Naïve Bayes Classifier for Numerical Attributes"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "source": [
    "# To find list of accuracy and MSE values of the model using K-fold cross validation\r\n",
    "\r\n",
    "from sklearn.model_selection import KFold\r\n",
    "\r\n",
    "kf = KFold(n_splits=10)\r\n",
    "\r\n",
    "print(\"============NAIVE BAYES CLASSIFIER FOR NUMERICAL ATTRIBUTES USING 10-FOLD CROSS VALIDATION=============\")\r\n",
    "\r\n",
    "idx = 0\r\n",
    "for train_indices, test_indices in kf.split(Numerical_X_encoded):\r\n",
    "    start_train, stop_train = train_indices[0], train_indices[-1]+1\r\n",
    "    start_test, stop_test = test_indices[0], test_indices[-1]+1\r\n",
    "    \r\n",
    "    GNB.fit(Numerical_X_encoded[start_train:stop_train], Y_encoded[start_train:stop_train])\r\n",
    "    pred = GNB.predict(Numerical_X_encoded[start_test:stop_test])\r\n",
    "\r\n",
    "    idx+=1\r\n",
    "    print(\"Accuracy for batch \", idx, \" : \", accuracy_score(Y_encoded[start_test:stop_test], pred))\r\n",
    "    print(\"Mean Square Error for batch \", idx, \" : \", mean_squared_error(Y_encoded[start_test:stop_test], pred))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "============NAIVE BAYES CLASSIFIER FOR NUMERICAL ATTRIBUTES USING 10-FOLD CROSS VALIDATION=============\n",
      "Accuracy for batch  1  :  0.7547169811320755\n",
      "Mean Square Error for batch  1  :  0.24528301886792453\n",
      "Accuracy for batch  2  :  0.8490566037735849\n",
      "Mean Square Error for batch  2  :  0.1509433962264151\n",
      "Accuracy for batch  3  :  0.7547169811320755\n",
      "Mean Square Error for batch  3  :  0.24528301886792453\n",
      "Accuracy for batch  4  :  0.7547169811320755\n",
      "Mean Square Error for batch  4  :  0.24528301886792453\n",
      "Accuracy for batch  5  :  0.8113207547169812\n",
      "Mean Square Error for batch  5  :  0.18867924528301888\n",
      "Accuracy for batch  6  :  0.7547169811320755\n",
      "Mean Square Error for batch  6  :  0.24528301886792453\n",
      "Accuracy for batch  7  :  0.8301886792452831\n",
      "Mean Square Error for batch  7  :  0.16981132075471697\n",
      "Accuracy for batch  8  :  0.8490566037735849\n",
      "Mean Square Error for batch  8  :  0.1509433962264151\n",
      "Accuracy for batch  9  :  0.8113207547169812\n",
      "Mean Square Error for batch  9  :  0.18867924528301888\n",
      "Accuracy for batch  10  :  0.8461538461538461\n",
      "Mean Square Error for batch  10  :  0.15384615384615385\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 5) Naïve Bayes Classifier for Categorical Attributes\r\n",
    "The Categorical Naïve Bayes Classifier handles only the categorical attributes and assumes a categoical distribution for each feature; where the categorical distribution is a generalization of the Bernoulli distribution for categorical random variables that are discrete. (https://www.inblog.in/Categorical-Naive-Bayes-Classifier-implementation-in-Python-dAVqLWkf7E)\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "source": [
    "from sklearn.metrics import classification_report\r\n",
    "from sklearn.naive_bayes import CategoricalNB\r\n",
    "\r\n",
    "print(\"============NAIVE BAYES CLASSIFIER FOR CATEGORICAL ATTRIBUTES=============\")\r\n",
    "\r\n",
    "NB = CategoricalNB() \r\n",
    "\r\n",
    "NB.fit(Categorical_X_train_encoded, Y_train_encoded)\r\n",
    "print(classification_report(Y_test_encoded, NB.predict(Categorical_X_test_encoded)))\r\n",
    "print(\"Confusion matrix for Categorical Naive Bayes Classifier: \")\r\n",
    "print(confusion_matrix(NB.predict(Categorical_X_test_encoded),Y_test_encoded))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "============NAIVE BAYES CLASSIFIER FOR CATEGORICAL ATTRIBUTES=============\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.47      0.19      0.27        42\n",
      "           1       0.62      0.86      0.72        64\n",
      "\n",
      "    accuracy                           0.59       106\n",
      "   macro avg       0.54      0.52      0.50       106\n",
      "weighted avg       0.56      0.59      0.54       106\n",
      "\n",
      "Confusion matrix for Categorical Naive Bayes Classifier: \n",
      "[[ 8  9]\n",
      " [34 55]]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 6) 10-fold cross validation to generalize the Naïve Bayes Classifier for Categorical Attributes"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "source": [
    "from sklearn.model_selection import KFold\r\n",
    "\r\n",
    "print(\"============NAIVE BAYES CLASSIFIER FOR CATEGORICAL ATTRIBUTES USING 10-FOLD CROSS VALIDATION=============\")\r\n",
    "\r\n",
    "kf = KFold(n_splits=10)\r\n",
    "\r\n",
    "idx = 0\r\n",
    "for train_indices, test_indices in kf.split(Categorical_X_encoded):\r\n",
    "    start_train, stop_train = train_indices[0], train_indices[-1]+1\r\n",
    "    start_test, stop_test = test_indices[0], test_indices[-1]+1\r\n",
    "    \r\n",
    "    GNB.fit(Categorical_X_encoded[start_train:stop_train], Y_encoded[start_train:stop_train])\r\n",
    "    pred = GNB.predict(Categorical_X_encoded[start_test:stop_test])\r\n",
    "\r\n",
    "    idx+=1\r\n",
    "    print(\"Accuracy for batch \", idx, \" : \", accuracy_score(Y_encoded[start_test:stop_test], pred))\r\n",
    "    print(\"Mean Square Error for batch \", idx, \" : \", mean_squared_error(Y_encoded[start_test:stop_test], pred))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "============NAIVE BAYES CLASSIFIER FOR CATEGORICAL ATTRIBUTES USING 10-FOLD CROSS VALIDATION=============\n",
      "Accuracy for batch  1  :  0.41509433962264153\n",
      "Mean Square Error for batch  1  :  0.5849056603773585\n",
      "Accuracy for batch  2  :  0.2830188679245283\n",
      "Mean Square Error for batch  2  :  0.7169811320754716\n",
      "Accuracy for batch  3  :  0.41509433962264153\n",
      "Mean Square Error for batch  3  :  0.5849056603773585\n",
      "Accuracy for batch  4  :  0.2830188679245283\n",
      "Mean Square Error for batch  4  :  0.7169811320754716\n",
      "Accuracy for batch  5  :  0.3018867924528302\n",
      "Mean Square Error for batch  5  :  0.6981132075471698\n",
      "Accuracy for batch  6  :  0.3584905660377358\n",
      "Mean Square Error for batch  6  :  0.6415094339622641\n",
      "Accuracy for batch  7  :  0.37735849056603776\n",
      "Mean Square Error for batch  7  :  0.6226415094339622\n",
      "Accuracy for batch  8  :  0.33962264150943394\n",
      "Mean Square Error for batch  8  :  0.660377358490566\n",
      "Accuracy for batch  9  :  0.32075471698113206\n",
      "Mean Square Error for batch  9  :  0.6792452830188679\n",
      "Accuracy for batch  10  :  0.34615384615384615\n",
      "Mean Square Error for batch  10  :  0.6538461538461539\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 7) Support Vector Machines\r\n",
    "Support Vector Machines are supervised learning algorithms that are well supported for classification and regression problems due to their significant accuracy. The goal of an SVM is to find decision boundaries, also known as hyperplanes, within an N-dimension plane (where N is the number of features) that help classify the samples given. The position and orientation of the hyperplane is influenced by support vectors, data points that are closer to the hyperplane, which help build the model. Since the loan prediction is a binary classification problem, using an SVM is a good approach to creating a predictive model.\r\n",
    "\r\n",
    "The first SVM uses a Gaussian (or radial basis function) kernel which is a non-linear SVM that uses the euclidian distance between two points to help classify the samples."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "source": [
    "from sklearn.svm import SVC, LinearSVC\r\n",
    "from sklearn import metrics\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "print(\"============ RBF Kernal =============\")\r\n",
    "clf_rbf = SVC(kernel='rbf')\r\n",
    "clf_rbf.fit(Combined_X_train_encoded, Y_train_encoded)\r\n",
    "print(\"Training: \")\r\n",
    "print(classification_report(Y_train_encoded, clf_rbf.predict(Combined_X_train_encoded)))\r\n",
    "print(\"Testing: \")\r\n",
    "print(classification_report(Y_test_encoded, clf_rbf.predict(Combined_X_test_encoded)))\r\n",
    "metrics.plot_roc_curve(clf_rbf, Combined_X_test_encoded, Y_test_encoded)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "============ RBF Kernal =============\n",
      "Training: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.40      0.55       121\n",
      "           1       0.80      0.98      0.88       302\n",
      "\n",
      "    accuracy                           0.81       423\n",
      "   macro avg       0.84      0.69      0.72       423\n",
      "weighted avg       0.82      0.81      0.79       423\n",
      "\n",
      "Testing: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.55      0.71        42\n",
      "           1       0.77      1.00      0.87        64\n",
      "\n",
      "    accuracy                           0.82       106\n",
      "   macro avg       0.89      0.77      0.79       106\n",
      "weighted avg       0.86      0.82      0.81       106\n",
      "\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<sklearn.metrics._plot.roc_curve.RocCurveDisplay at 0x20a76ee9250>"
      ]
     },
     "metadata": {},
     "execution_count": 193
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The second SVM is a linear SVM which removes all of the data points that are not closer to the hyperplane, resulting with only support vectors."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(\"============ Linear Kernal =============\")\r\n",
    "clf_lin = LinearSVC(dual=False)\r\n",
    "clf_lin.fit(Combined_X_train_encoded, Y_train_encoded)\r\n",
    "print(\"Training: \")\r\n",
    "print(classification_report(Y_train_encoded, clf_lin.predict(Combined_X_train_encoded)))\r\n",
    "print(\"Testing: \")\r\n",
    "print(classification_report(Y_test_encoded, clf_lin.predict(Combined_X_test_encoded)))\r\n",
    "metrics.plot_roc_curve(clf_lin, Combined_X_test_encoded, Y_test_encoded)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Both of the SVMs have similar results in terms of accuracy, however the Linear SVM is a better candidate for this problem than the RBF SVM as shown in the ROC curve. The ROC Curve of the Linear SVM is closer to the Y axis for majority of the plot compared to the RBF SVM, showing a more efficient performance from the Linear SVM.\r\n",
    "\r\n",
    "This could be due to the large dataset which increases the complexity of non-linear SVMs. On the other hand, the linear SVM will not increase in complexity with larger datasets."
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.5 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "interpreter": {
   "hash": "7cae688b6d5c7886c97c16c08b8426e0c75b8da6e1119f020936594eaecaf1a0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}